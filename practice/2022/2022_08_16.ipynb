{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab74e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbce11f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m cost \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(hypothesis \u001b[38;5;241m-\u001b[39mY))\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mGradientDescentOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m sess \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mSession()\n\u001b[1;32m     16\u001b[0m sess\u001b[38;5;241m.\u001b[39mrun(tf\u001b[38;5;241m.\u001b[39mglobal_variables_initializer())\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/training/optimizer.py:477\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, var_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m              gate_gradients\u001b[38;5;241m=\u001b[39mGATE_OP, aggregation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    435\u001b[0m              colocate_gradients_with_ops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    436\u001b[0m              grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    437\u001b[0m   \u001b[38;5;124;03m\"\"\"Add operations to minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m  This method simply combines calls `compute_gradients()` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgate_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgate_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcolocate_gradients_with_ops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolocate_gradients_with_ops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m   vars_with_grad \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vars_with_grad:\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/training/optimizer.py:575\u001b[0m, in \u001b[0;36mOptimizer.compute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Non-callable/Tensor loss case\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 575\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    576\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` passed to Optimizer.compute_gradients should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe a function when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Scale loss if using a \"mean\" loss reduction and multiple replicas.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_loss(loss)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "hypothesis = X * W\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis -Y))\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step, sess.run(W))\n",
    "    sess.run(train)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd38ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
